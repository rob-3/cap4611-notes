# Problems with Data

- Not enough: results from math won't be good
- Not representative: dataset used to train must resemble the intended use
- Poor quality data: dataset must include all important features or it will find erroneous relationship
- Data doesn't matter: if there's no relationship, you'll get bad results
- Ex: (x, y) = (age, height): f(age) = height
- Underfitting: model doesn't fit the data well enough
- Overfitting: model fits the data so well that it makes erroneous predictions by "reading into it" too much
- Training data: use subset of data to build model and use the rest as...
- Test data: used to test the model
  > If your model is bad, you need to go back and resample your total data to reform these sets. Otherwise, you're using info from the test data to inform your results
  > Error == Divergence == Loss
- Error = \sum (yhat - y)^2
- Parameter: adjusted in model during training

# Hyperparameters

- Hyperparameter: adjust between model runs
- Break data into 3 subsets: training, test, and validation
- Basically an optimization over the process of resampling that reduces potential overfitting

# Cross Validation
- Automates the hyperparameter adjustment process
- Back to 2 subsets: validation and training
- Train model on 4/5 of data and validate on 1/5; repeat this over each fifth of the data
    - Fivefold cross validation
- Called K-fold cross validation in general
- If using n-1 folds, "leave one out" validation
